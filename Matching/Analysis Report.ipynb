{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Report\n",
    "## Files\n",
    "setC.csv = data obtained from the blocking stage  \n",
    "sampleA.csv = 800 rows that are sample with (seed = 10) from setC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "setC = None\n",
    "with open('labelled.csv', 'r') as file:\n",
    "    setA = list(csv.reader(file))\n",
    "    setA = setA[1:]\n",
    "\n",
    "class attr:\n",
    "    label = 0\n",
    "    _id = 1\n",
    "    ltable_Id = 2\n",
    "    rtable_Id = 3\n",
    "    ltable_Title = 4\n",
    "    ltable_Category = 5\n",
    "    ltable_Duration = 6\n",
    "    ltable_Rating = 7\n",
    "    ltable_Rating_Count = 8\n",
    "    ltable_Director = 9 \n",
    "    rtable_Title = 10\n",
    "    rtable_Category = 11\n",
    "    rtable_Duration = 12\n",
    "    rtable_Rating = 13\n",
    "    rtable_Rating_Count = 14\n",
    "    rtable_Director = 15\n",
    "    strings = ['label', '_id', 'ltable_Id', 'rtable_Id', 'ltable_Title', 'ltable_Category', \n",
    "               'ltable_Duration', 'ltable_Rating', 'ltable_Rating_Count', 'ltable_Director', \n",
    "               'rtable_Title', 'rtable_Category', 'rtable_Duration', 'rtable_Rating', 'rtable_Rating_Count',\n",
    "               'rtable_Director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate number of null value for each attributes\n",
    "def check_null(setx):\n",
    "    num_null = [0 for i in range(16)]\n",
    "    \n",
    "    for row in setx:\n",
    "        for pos, val in enumerate(row):\n",
    "            if not val:\n",
    "                num_null[pos] += 1\n",
    "    \n",
    "    for pos, val in enumerate(num_null):\n",
    "        print(attr.strings[pos] + \": \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that scan the whole table and remove null value based on pos\n",
    "def fill_null(setx, pos, val):\n",
    "    for row in setx:\n",
    "        if not row[pos]:\n",
    "            row[pos] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n",
      "_id: 0\n",
      "ltable_Id: 0\n",
      "rtable_Id: 0\n",
      "ltable_Title: 0\n",
      "ltable_Category: 6\n",
      "ltable_Duration: 0\n",
      "ltable_Rating: 57\n",
      "ltable_Rating_Count: 57\n",
      "ltable_Director: 151\n",
      "rtable_Title: 0\n",
      "rtable_Category: 28\n",
      "rtable_Duration: 86\n",
      "rtable_Rating: 132\n",
      "rtable_Rating_Count: 0\n",
      "rtable_Director: 60\n"
     ]
    }
   ],
   "source": [
    "# Values with null item, size of setA = 800\n",
    "check_null(setA)\n",
    "fill_null(setA, attr.ltable_Rating, 0)\n",
    "fill_null(setA, attr.rtable_Rating, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Matching\n",
    "Start by converting each labelled row into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree, ensemble, linear_model, svm, naive_bayes\n",
    "from sklearn.model_selection import KFold\n",
    "from py_stringmatching.tokenizer.delimiter_tokenizer import DelimiterTokenizer\n",
    "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
    "\n",
    "delim_tkn = DelimiterTokenizer()\n",
    "lev = Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title_match(tit_x, tit_y):\n",
    "    return lev.get_raw_score(tit_x, tit_y)\n",
    "\n",
    "def category_match(cat_x, cat_y):\n",
    "    return lev.get_raw_score(cat_x, cat_y)\n",
    "    \n",
    "def rating_match(rat_x, rat_y):\n",
    "    return abs(float(rat_x) - float(rat_y))\n",
    "    \n",
    "def director_match(dir_x, dir_y):\n",
    "    return lev.get_raw_score(dir_x, dir_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(setx):\n",
    "    feature = []\n",
    "    label = []\n",
    "    \n",
    "    for row in setx:\n",
    "        label += [row[attr.label]]\n",
    "        \n",
    "        x_0 = title_match(row[attr.ltable_Title], row[attr.rtable_Title])\n",
    "        x_1 = category_match(row[attr.ltable_Category], row[attr.rtable_Category])\n",
    "        x_2 = rating_match(row[attr.ltable_Rating], row[attr.rtable_Rating])\n",
    "        x_3 = director_match(row[attr.ltable_Director], row[attr.rtable_Director])\n",
    "        \n",
    "        feature += [[x_0, x_1, x_2, x_3]]\n",
    "        \n",
    "    return feature, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ltable(setx):\n",
    "    return [[row[attr.ltable_Id]] + row[attr.ltable_Title:attr.ltable_Director + 1]for row in setx]\n",
    "\n",
    "def get_rtable(setx):\n",
    "    return [[row[attr.rtable_Id]] + row[attr.rtable_Title:]for row in setx]\n",
    "\n",
    "def get_label(setx):\n",
    "    return [row[attr.label] for row in setx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of real result and predicted result, calculate precision, recall and F1\n",
    "def get_F1(real, predicted):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res == real[pos]:\n",
    "            if res:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        else:\n",
    "            if res == '1':\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "                \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def debug(ltable, rtable, label, predicted):\n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res != label[pos]:\n",
    "            print(\"ltable: \" + str(ltable[pos]))\n",
    "            print(\"rtable: \" + str(rtable[pos]))\n",
    "            print(\"Label: \" + str(label[pos]) + \" Predicted: \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.9512195121951219\n",
      "recall: 0.9831932773109243\n",
      "F1: 0.9669421487603305\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 0.9426229508196722\n",
      "recall: 0.9829059829059829\n",
      "F1: 0.9623430962343097\n",
      "AVERAGE:\n",
      "precision: 0.9734606157536985\n",
      "recall: 0.9915248150542268\n",
      "F1: 0.9823213112486601\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    t_clf = tree.DecisionTreeClassifier()\n",
    "    t_clf = t_clf.fit(feature, label)\n",
    "\n",
    "    result = t_clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.975609756097561\n",
      "recall: 0.9836065573770492\n",
      "F1: 0.9795918367346939\n",
      "ROUND: 1\n",
      "precision: 0.9752066115702479\n",
      "recall: 0.9672131147540983\n",
      "F1: 0.9711934156378601\n",
      "ROUND: 2\n",
      "precision: 0.9916666666666667\n",
      "recall: 0.9596774193548387\n",
      "F1: 0.9754098360655739\n",
      "ROUND: 3\n",
      "precision: 0.9754098360655737\n",
      "recall: 0.9834710743801653\n",
      "F1: 0.9794238683127573\n",
      "AVERAGE:\n",
      "precision: 0.9794732176000123\n",
      "recall: 0.9734920414665379\n",
      "F1: 0.9764047391877213\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.983739837398374\n",
      "recall: 0.983739837398374\n",
      "F1: 0.983739837398374\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 0.9672131147540983\n",
      "recall: 0.9833333333333333\n",
      "F1: 0.9752066115702478\n",
      "AVERAGE:\n",
      "precision: 0.9877382380381181\n",
      "recall: 0.9917682926829269\n",
      "F1: 0.9897366122421555\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = ensemble.RandomForestClassifier()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 1.0\n",
      "recall: 0.896\n",
      "F1: 0.9451476793248946\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 1.0\n",
      "recall: 0.9354838709677419\n",
      "F1: 0.9666666666666666\n",
      "AVERAGE:\n",
      "precision: 1.0\n",
      "recall: 0.9578709677419355\n",
      "F1: 0.9779535864978903\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.975609756097561\n",
      "recall: 0.9836065573770492\n",
      "F1: 0.9795918367346939\n",
      "ROUND: 1\n",
      "precision: 0.9752066115702479\n",
      "recall: 0.9672131147540983\n",
      "F1: 0.9711934156378601\n",
      "ROUND: 2\n",
      "precision: 0.9831932773109243\n",
      "recall: 0.9512195121951219\n",
      "F1: 0.9669421487603305\n",
      "ROUND: 3\n",
      "precision: 0.9262295081967213\n",
      "recall: 0.9826086956521739\n",
      "F1: 0.9535864978902953\n",
      "AVERAGE:\n",
      "precision: 0.9650597882938636\n",
      "recall: 0.9711619699946108\n",
      "F1: 0.9678284747557949\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = naive_bayes.GaussianNB()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
