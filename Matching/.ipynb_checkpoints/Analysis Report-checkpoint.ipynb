{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Report\n",
    "## Files\n",
    "setC.csv = data obtained from the blocking stage  \n",
    "sampleA.csv = 800 rows that are sample with (seed = 10) from setC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "setC = None\n",
    "with open('labelled.csv', 'r') as file:\n",
    "    setA = list(csv.reader(file))\n",
    "    setA = setA[1:] # Remove header\n",
    "    setB = setA[350:] # Evaluation set\n",
    "    setA = setA[:350] # Development set\n",
    "\n",
    "class attr:\n",
    "    label = 0\n",
    "    _id = 1\n",
    "    ltable_Id = 2\n",
    "    rtable_Id = 3\n",
    "    ltable_Title = 4\n",
    "    ltable_Category = 5\n",
    "    ltable_Duration = 6\n",
    "    ltable_Rating = 7\n",
    "    ltable_Rating_Count = 8\n",
    "    ltable_Director = 9 \n",
    "    rtable_Title = 10\n",
    "    rtable_Category = 11\n",
    "    rtable_Duration = 12\n",
    "    rtable_Rating = 13\n",
    "    rtable_Rating_Count = 14\n",
    "    rtable_Director = 15\n",
    "    strings = ['label', '_id', 'ltable_Id', 'rtable_Id', 'ltable_Title', 'ltable_Category', \n",
    "               'ltable_Duration', 'ltable_Rating', 'ltable_Rating_Count', 'ltable_Director', \n",
    "               'rtable_Title', 'rtable_Category', 'rtable_Duration', 'rtable_Rating', 'rtable_Rating_Count',\n",
    "               'rtable_Director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate number of null value for each attributes\n",
    "def check_null(setx):\n",
    "    num_null = [0 for i in range(16)]\n",
    "    \n",
    "    for row in setx:\n",
    "        for pos, val in enumerate(row):\n",
    "            if not val:\n",
    "                num_null[pos] += 1\n",
    "    \n",
    "    for pos, val in enumerate(num_null):\n",
    "        print(attr.strings[pos] + \": \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that scan the whole table and remove null value based on pos\n",
    "def fill_null(setx, pos, val):\n",
    "    for row in setx:\n",
    "        if not row[pos]:\n",
    "            row[pos] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetA\n",
      "label: 0\n",
      "_id: 0\n",
      "ltable_Id: 0\n",
      "rtable_Id: 0\n",
      "ltable_Title: 0\n",
      "ltable_Category: 5\n",
      "ltable_Duration: 0\n",
      "ltable_Rating: 39\n",
      "ltable_Rating_Count: 39\n",
      "ltable_Director: 107\n",
      "rtable_Title: 0\n",
      "rtable_Category: 18\n",
      "rtable_Duration: 61\n",
      "rtable_Rating: 93\n",
      "rtable_Rating_Count: 0\n",
      "rtable_Director: 44\n",
      "SetB\n",
      "label: 0\n",
      "_id: 0\n",
      "ltable_Id: 0\n",
      "rtable_Id: 0\n",
      "ltable_Title: 0\n",
      "ltable_Category: 1\n",
      "ltable_Duration: 0\n",
      "ltable_Rating: 18\n",
      "ltable_Rating_Count: 18\n",
      "ltable_Director: 44\n",
      "rtable_Title: 0\n",
      "rtable_Category: 10\n",
      "rtable_Duration: 25\n",
      "rtable_Rating: 39\n",
      "rtable_Rating_Count: 0\n",
      "rtable_Director: 16\n"
     ]
    }
   ],
   "source": [
    "# Values with null item, size of setA = 800\n",
    "print(\"SetA\")\n",
    "check_null(setA)\n",
    "fill_null(setA, attr.ltable_Rating, 0)\n",
    "fill_null(setA, attr.rtable_Rating, 0)\n",
    "print(\"SetB\")\n",
    "check_null(setB)\n",
    "fill_null(setB, attr.ltable_Rating, 0)\n",
    "fill_null(setB, attr.rtable_Rating, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Matching\n",
    "Start by converting each labelled row into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree, ensemble, linear_model, svm, naive_bayes\n",
    "from sklearn.model_selection import KFold\n",
    "from py_stringmatching.tokenizer.delimiter_tokenizer import DelimiterTokenizer\n",
    "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
    "\n",
    "delim_tkn = DelimiterTokenizer()\n",
    "lev = Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title_match(tit_x, tit_y):\n",
    "#     return len(tit_x) - len(tit_y)\n",
    "    return lev.get_raw_score(tit_x, tit_y)\n",
    "\n",
    "def category_match(cat_x, cat_y):\n",
    "#     return len(cat_x) - len(cat_y)\n",
    "    return lev.get_raw_score(cat_x, cat_y)\n",
    "    \n",
    "def rating_match(rat_x, rat_y):\n",
    "    return abs(float(rat_x) - float(rat_y))\n",
    "    \n",
    "def director_match(dir_x, dir_y):\n",
    "#     return len(dir_x) - len(dir_y)\n",
    "    return lev.get_raw_score(dir_x, dir_y)\n",
    "\n",
    "def rating_count_match(rat_x, rat_y):\n",
    "    return abs(float(rat_x) - float(rat_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(setx):\n",
    "    feature = []\n",
    "    label = []\n",
    "    \n",
    "    for row in setx:\n",
    "        label += [row[attr.label]]\n",
    "        \n",
    "        x_0 = title_match(row[attr.ltable_Title], row[attr.rtable_Title])\n",
    "        x_1 = category_match(row[attr.ltable_Category], row[attr.rtable_Category])\n",
    "        x_2 = rating_match(row[attr.ltable_Rating], row[attr.rtable_Rating])\n",
    "        x_3 = director_match(row[attr.ltable_Director], row[attr.rtable_Director])\n",
    "#         x_4 = rating_count_match(row[attr.ltable_Rating_Count], row[attr.rtable_Rating_Count])\n",
    "        \n",
    "        feature += [[x_0, x_1, x_2, x_3]]\n",
    "        \n",
    "    return feature, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ltable(setx):\n",
    "    return [[row[attr.ltable_Id]] + row[attr.ltable_Title:attr.ltable_Director + 1]for row in setx]\n",
    "\n",
    "def get_rtable(setx):\n",
    "    return [[row[attr.rtable_Id]] + row[attr.rtable_Title:]for row in setx]\n",
    "\n",
    "def get_label(setx):\n",
    "    return [row[attr.label] for row in setx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of real result and predicted result, calculate precision, recall and F1\n",
    "def get_F1(real, predicted):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res == real[pos]:\n",
    "            if res == '1':\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        else:\n",
    "            if res == '1':\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "                \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def debug(ltable, rtable, label, predicted):\n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res != label[pos]:\n",
    "            print(\"ltable: \" + str(ltable[pos]))\n",
    "            print(\"rtable: \" + str(rtable[pos]))\n",
    "            print(\"Label: \" + str(label[pos]) + \" Predicted: \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.9230769230769231\n",
      "recall: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 0.8387096774193549\n",
      "recall: 0.9629629629629629\n",
      "F1: 0.896551724137931\n",
      "AVERAGE:\n",
      "precision: 0.9404466501240696\n",
      "recall: 0.9907407407407407\n",
      "F1: 0.9641379310344828\n",
      "1.0\n",
      "0.7954545454545454\n",
      "0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "    \n",
    "    t_clf = tree.DecisionTreeClassifier()\n",
    "    t_clf = t_clf.fit(feature, label)\n",
    "\n",
    "    result = t_clf.predict(feature_t)\n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))\n",
    "\n",
    "eval_f, eval_t = get_feature(setB)\n",
    "rest = clf.predict(eval_f)\n",
    "p,r,f = get_F1(eval_t, rest)\n",
    "\n",
    "print(p)\n",
    "print(r)\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.9230769230769231\n",
      "recall: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROUND: 1\n",
      "precision: 0.9655172413793104\n",
      "recall: 0.8484848484848485\n",
      "F1: 0.9032258064516129\n",
      "ROUND: 2\n",
      "precision: 0.8888888888888888\n",
      "recall: 0.96\n",
      "F1: 0.923076923076923\n",
      "ROUND: 3\n",
      "precision: 1.0\n",
      "recall: 0.8518518518518519\n",
      "F1: 0.92\n",
      "AVERAGE:\n",
      "precision: 0.9443707633362806\n",
      "recall: 0.9150841750841752\n",
      "F1: 0.926575682382134\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.8888888888888888\n",
      "recall: 1.0\n",
      "F1: 0.9411764705882353\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 0.9393939393939394\n",
      "F1: 0.96875\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 0.9259259259259259\n",
      "recall: 0.9259259259259259\n",
      "F1: 0.9259259259259259\n",
      "AVERAGE:\n",
      "precision: 0.9537037037037037\n",
      "recall: 0.9663299663299664\n",
      "F1: 0.9589630991285404\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = ensemble.RandomForestClassifier()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 1.0\n",
      "recall: 0.6666666666666666\n",
      "F1: 0.8\n",
      "ROUND: 1\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 2\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1: 1.0\n",
      "ROUND: 3\n",
      "precision: 1.0\n",
      "recall: 0.5555555555555556\n",
      "F1: 0.7142857142857143\n",
      "AVERAGE:\n",
      "precision: 1.0\n",
      "recall: 0.8055555555555556\n",
      "F1: 0.8785714285714286\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND: 0\n",
      "precision: 0.8275862068965517\n",
      "recall: 1.0\n",
      "F1: 0.9056603773584906\n",
      "ROUND: 1\n",
      "precision: 0.9032258064516129\n",
      "recall: 0.8484848484848485\n",
      "F1: 0.875\n",
      "ROUND: 2\n",
      "precision: 0.8571428571428571\n",
      "recall: 0.96\n",
      "F1: 0.9056603773584904\n",
      "ROUND: 3\n",
      "precision: 1.0\n",
      "recall: 0.8518518518518519\n",
      "F1: 0.92\n",
      "AVERAGE:\n",
      "precision: 0.8969887176227554\n",
      "recall: 0.9150841750841752\n",
      "F1: 0.9015801886792452\n",
      "0.8163265306122449\n",
      "0.9090909090909091\n",
      "0.8602150537634408\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian Classifier using k-Fold = 4\n",
    "split = 4\n",
    "k_fold = KFold(n_splits=split)\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_F1 = 0\n",
    "x = 0\n",
    "\n",
    "for train, test in k_fold.split(setA):\n",
    "    train = setA[train[0]:train[-1] + 1]\n",
    "    test = setA[test[0]:test[-1] + 1]\n",
    "    \n",
    "    feature, label = get_feature(train)\n",
    "    feature_t, label_t = get_feature(test)\n",
    "\n",
    "    clf = naive_bayes.GaussianNB()\n",
    "    clf = clf.fit(feature, label)\n",
    "\n",
    "    result = clf.predict(feature_t)\n",
    "    \n",
    "    precision, recall, F1 = get_F1(label_t, result)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_F1 += F1\n",
    "    \n",
    "    print(\"ROUND: \" + str(x))\n",
    "    x += 1\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"F1: \" + str(F1))\n",
    "#     debug(get_ltable(test), get_rtable(test), label_t, result)\n",
    "    \n",
    "print(\"AVERAGE:\")\n",
    "precision = total_precision/split\n",
    "recall = total_recall/split\n",
    "F1 = total_F1/split\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"F1: \" + str(F1))\n",
    "\n",
    "eval_f, eval_t = get_feature(setB)\n",
    "rest = clf.predict(eval_f)\n",
    "p,r,f = get_F1(eval_t, rest)\n",
    "\n",
    "print(p)\n",
    "print(r)\n",
    "print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
