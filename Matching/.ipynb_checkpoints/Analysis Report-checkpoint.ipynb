{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Report\n",
    "## Files\n",
    "setC.csv = data obtained from the blocking stage  \n",
    "sampleA.csv = 800 rows that are sample with (seed = 10) from setC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "setC = None\n",
    "with open('labelled.csv', 'r') as file:\n",
    "    setA = list(csv.reader(file))\n",
    "    setA = setA[1:] # Remove header\n",
    "    setB = setA[350:] # Evaluation set\n",
    "    setA = setA[:350] # Development set\n",
    "\n",
    "class attr:\n",
    "    label = 0\n",
    "    _id = 1\n",
    "    ltable_Id = 2\n",
    "    rtable_Id = 3\n",
    "    ltable_Title = 4\n",
    "    ltable_Category = 5\n",
    "    ltable_Duration = 6\n",
    "    ltable_Rating = 7\n",
    "    ltable_Rating_Count = 8\n",
    "    ltable_Director = 9 \n",
    "    rtable_Title = 10\n",
    "    rtable_Category = 11\n",
    "    rtable_Duration = 12\n",
    "    rtable_Rating = 13\n",
    "    rtable_Rating_Count = 14\n",
    "    rtable_Director = 15\n",
    "    strings = ['label', '_id', 'ltable_Id', 'rtable_Id', 'ltable_Title', 'ltable_Category', \n",
    "               'ltable_Duration', 'ltable_Rating', 'ltable_Rating_Count', 'ltable_Director', \n",
    "               'rtable_Title', 'rtable_Category', 'rtable_Duration', 'rtable_Rating', 'rtable_Rating_Count',\n",
    "               'rtable_Director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate number of null value for each attributes\n",
    "def check_null(setx):\n",
    "    num_null = [0 for i in range(16)]\n",
    "    \n",
    "    for row in setx:\n",
    "        for pos, val in enumerate(row):\n",
    "            if not val:\n",
    "                num_null[pos] += 1\n",
    "    \n",
    "    for pos, val in enumerate(num_null):\n",
    "        print(attr.strings[pos] + \": \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that scan the whole table and remove null value based on pos\n",
    "def fill_null(setx, pos, val):\n",
    "    for row in setx:\n",
    "        if not row[pos]:\n",
    "            row[pos] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Values with null item, size of setA = 800\n",
    "# print(\"SetA\")\n",
    "# check_null(setA)\n",
    "fill_null(setA, attr.ltable_Rating, 0)\n",
    "fill_null(setA, attr.rtable_Rating, 0)\n",
    "# print(\"SetB\")\n",
    "# check_null(setB)\n",
    "fill_null(setB, attr.ltable_Rating, 0)\n",
    "fill_null(setB, attr.rtable_Rating, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Matching\n",
    "Start by converting each labelled row into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree, ensemble, linear_model, svm, naive_bayes\n",
    "from sklearn.model_selection import KFold\n",
    "from py_stringmatching.tokenizer.delimiter_tokenizer import DelimiterTokenizer\n",
    "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
    "\n",
    "delim_tkn = DelimiterTokenizer()\n",
    "lev = Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title_match(tit_x, tit_y):\n",
    "    return lev.get_raw_score(tit_x, tit_y)\n",
    "\n",
    "def category_match(cat_x, cat_y):\n",
    "    return lev.get_raw_score(cat_x, cat_y)\n",
    "    \n",
    "def rating_match(rat_x, rat_y):\n",
    "    return abs(float(rat_x) - float(rat_y))\n",
    "    \n",
    "def director_match(dir_x, dir_y):\n",
    "    return lev.get_raw_score(dir_x, dir_y)\n",
    "\n",
    "def rating_count_match(rat_x, rat_y):\n",
    "    return abs(float(rat_x) - float(rat_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(setx):\n",
    "    feature = []\n",
    "    label = []\n",
    "    \n",
    "    for row in setx:\n",
    "        label += [row[attr.label]]\n",
    "        \n",
    "        x_0 = title_match(row[attr.ltable_Title], row[attr.rtable_Title])\n",
    "        x_1 = category_match(row[attr.ltable_Category], row[attr.rtable_Category])\n",
    "        x_2 = rating_match(row[attr.ltable_Rating], row[attr.rtable_Rating])\n",
    "        x_3 = director_match(row[attr.ltable_Director], row[attr.rtable_Director])\n",
    "#         x_4 = rating_count_match(row[attr.ltable_Rating_Count], row[attr.rtable_Rating_Count])\n",
    "        \n",
    "        feature += [[x_0, x_1, x_2, x_3]]\n",
    "        \n",
    "    return feature, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ltable(setx):\n",
    "    return [[row[attr.ltable_Id]] + row[attr.ltable_Title:attr.ltable_Director + 1]for row in setx]\n",
    "\n",
    "def get_rtable(setx):\n",
    "    return [[row[attr.rtable_Id]] + row[attr.rtable_Title:]for row in setx]\n",
    "\n",
    "def get_label(setx):\n",
    "    return [row[attr.label] for row in setx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a list of real result and predicted result, calculate precision, recall and F1\n",
    "def get_F1(real, predicted):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res == real[pos]:\n",
    "            if res == '1':\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        else:\n",
    "            if res == '1':\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "    \n",
    "    # If true_positive, false_positive or false_negative causes zero error: set precision, recall and F1 to zero\n",
    "    try:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        F1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return 0,0,0\n",
    "    \n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def debug(ltable, rtable, label, predicted):\n",
    "    for pos,res in enumerate(predicted):\n",
    "        if res != label[pos]:\n",
    "            print(\"ltable: \" + str(ltable[pos]))\n",
    "            print(\"rtable: \" + str(rtable[pos]))\n",
    "            print(\"Label: \" + str(label[pos]) + \" Predicted: \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test setx using the classifier = clf\n",
    "def clf_test(setx, clf, test_name='TEST', verbose=False):\n",
    "    feature, label = get_feature(setx)\n",
    "    result = clf.predict(feature)\n",
    "    precision, recall, F1 = get_F1(label, result)\n",
    "    \n",
    "    if verbose:\n",
    "        print(test_name.upper())\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"F1: \" + str(F1))\n",
    "    \n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and test on setx using the classifier = clf and k-fold validation = k\n",
    "# Return the average (precision, recall, F1)\n",
    "def clf_train(setx, clf, k, name='CLASSIFIER', verbose=False):\n",
    "    # Decision Tree Classifier using k-Fold = 4\n",
    "    split = 4\n",
    "    k_fold = KFold(n_splits=split)\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_F1 = 0\n",
    "    \n",
    "    for train, test in k_fold.split(setx):\n",
    "        train = setx[train[0]:train[-1] + 1]\n",
    "        test = setx[test[0]:test[-1] + 1]\n",
    "\n",
    "        feature, label = get_feature(train)\n",
    "        clf = clf.fit(feature, label)\n",
    "        \n",
    "        precision, recall, F1 = clf_test(test, clf)\n",
    "\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_F1 += F1\n",
    "        \n",
    "    precision = total_precision/split\n",
    "    recall = total_recall/split\n",
    "    F1 = total_F1/split\n",
    "    if verbose:\n",
    "        print(name.upper())\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"F1: \" + str(F1))\n",
    "    \n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER\n",
      "Precision: 0.931899641577061\n",
      "Recall: 0.9907407407407407\n",
      "F1: 0.9594320486815416\n",
      "\n",
      "EVALUATION ON DECISION TREE CLASSIFIER\n",
      "Precision: 0.7454545454545455\n",
      "Recall: 0.9318181818181818\n",
      "F1: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier using k-Fold = 4\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "precision, recall, F1 = clf_train(setA, clf, 4, \"decision tree classifier\", verbose=True)\n",
    "print()\n",
    "precision_b, recall_b, F1_b = clf_test(setB, clf, \"evaluation on decision tree classifier\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION CLASSIFIER\n",
      "Precision: 0.9443707633362806\n",
      "Recall: 0.9150841750841752\n",
      "F1: 0.926575682382134\n",
      "\n",
      "EVALUATION ON LOGISTIC REGRESSION CLASSIFIER\n",
      "Precision: 0.9090909090909091\n",
      "Recall: 0.9090909090909091\n",
      "F1: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier using k-Fold = 4\n",
    "clf = linear_model.LogisticRegression()\n",
    "precision, recall, F1 = clf_train(setA, clf, 4, \"logistic regression classifier\", verbose=True)\n",
    "print()\n",
    "precision_b, recall_b, F1_b = clf_test(setB, clf, \"evaluation on logistic regression classifier\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "Precision: 0.98\n",
      "Recall: 0.9803240740740741\n",
      "F1: 0.979976896418945\n",
      "\n",
      "EVALUATION ON RANDOM FOREST CLASSIFIER\n",
      "Precision: 0.9130434782608695\n",
      "Recall: 0.9545454545454546\n",
      "F1: 0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier using k-Fold = 4\n",
    "clf = ensemble.RandomForestClassifier()\n",
    "precision, recall, F1 = clf_train(setA, clf, 4, \"random forest classifier\", verbose=True)\n",
    "print()\n",
    "precision_b, recall_b, F1_b = clf_test(setB, clf, \"evaluation on random forest classifier\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORT VECTOR MACHINE CLASSIFIER\n",
      "Precision: 1.0\n",
      "Recall: 0.8055555555555556\n",
      "F1: 0.8785714285714286\n",
      "\n",
      "EVALUATION ON SUPPORT VECTOR MACHINE CLASSIFIER\n",
      "Precision: 1.0\n",
      "Recall: 0.5681818181818182\n",
      "F1: 0.7246376811594203\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine Classifier using k-Fold = 4\n",
    "clf = svm.SVC()\n",
    "precision, recall, F1 = clf_train(setA, clf, 4, \"support vector machine classifier\", verbose=True)\n",
    "print()\n",
    "precision_b, recall_b, F1_b = clf_test(setB, clf, \"evaluation on support vector machine classifier\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES CLASSIFIER\n",
      "Precision: 0.8969887176227554\n",
      "Recall: 0.9150841750841752\n",
      "F1: 0.9015801886792452\n",
      "\n",
      "EVALUATION ON NAIVE BAYES CLASSIFIER\n",
      "Precision: 0.8163265306122449\n",
      "Recall: 0.9090909090909091\n",
      "F1: 0.8602150537634408\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier using k-Fold = 4\n",
    "clf = naive_bayes.GaussianNB()\n",
    "precision, recall, F1 = clf_train(setA, clf, 4, \"naive bayes classifier\", verbose=True)\n",
    "print()\n",
    "precision_b, recall_b, F1_b = clf_test(setB, clf, \"evaluation on naive bayes classifier\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
